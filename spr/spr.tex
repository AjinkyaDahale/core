\documentclass{article}
\usepackage{amsmath}
\usepackage{algpseudocode}
\usepackage[margin=1.0in]{geometry}
\title{SPR-based Error Estimation}
\author{Dan Ibanez}

\begin{document}
\maketitle

\section{Overview}
An SPR-based error estimation procedure is defined in detail
in Jie Wan's thesis.
A similar implementation exists in old code which is currently
supporting both the SLAC electromagnetic simulation as well
as the Albany elastic simulation.
This document exists to specify precisely what this error
estimator is doing for the purposes of reimplementing it
with new libraries and identifying areas that need refinement.

From a high level, the SPR-based error estimator goes through
the following steps given a $p$-order Lagrangian field $f$:
\begin{enumerate}
\item Compute the $(p-1)$-order gradient field $\epsilon = \nabla f$
\item Recover a $p$-order gradient field $\epsilon^*$ using SPR
\item Integrate norms of the $p$-order error field
$e_\epsilon = \epsilon - \epsilon^*$
over elements and the whole mesh
\item Compute a linear isotropic (scalar) size field
based on the size factor formula for elements
\end{enumerate}
For various reasons, this method is only clear
when applied to simplex meshes.
We can describe each step in detail: (1) gradient computation,
(2) gradient recovery, (3) error estimation, (4) size field
computation.
In current uses, the field $f$ is a vector field in 3-space.

\section{Gradient Computation}
Wan uses a linear strain tensor as the field to be recovered.
The current implementation just uses the 3x3 derivative
of the input vector field $\nabla f$.
This field can be computed and stored using shape functions
of one lower polynomial order than those used for the input field.
Current users give only linear input fields, so the gradient
is a step-wise field.

\section{Gradient Recovery}
A closer approximation to the true gradient field is found by applying
superconvergent patch recovery to the direct gradient field.
Essentially, the recovered value at a node is found by fitting
a polynomial to the direct values at integration points around
that node.

The main item of concern is that SPR requires information from
an unpredictable number of elements adjacent to the target node.
Nodes near or on a boundary may require second or higher
order adjacencies to find enough elements.
For geometric boundaries this is natural and inevitable.
However, we would want parallel part boundaries to not affect the
numerical results.

Ghosting would provide the needed information in \emph{most} cases,
but the thesis makes it clear that SPR is not always bounded
to 1 layer of adjacent elements, so some thought is still required here.
The current code just queries adjacencies and receives what is available
from the interior mesh plus ghosted elements.

SPR is mainly a technique for recovering a value at a single node point.
The recovered field $\epsilon^*$ takes as its
nodal values the results of applying SPR around each node.

\section{Error Estimation}
A per-element scalar error estimate
$\|e_\epsilon\|_e$ is computed using the
norm of the integrated difference between
direct and recovered gradient fields.
\[e_\epsilon=\epsilon - \epsilon^*\]
The current implementation uses entry-wise L2 norms on
3x3 matrices, which seems to agree with Wan's thesis.
The following notations describe the L2 norm integration
over an element and over the whole mesh:
\[\|A\|_e=
\left(
\int_{\Omega^e} A : A d\Omega
\right)^\frac12,
\|A\|=
\left(
\sum_{e=1}^{n_e}
\int_{\Omega^e} A : A d\Omega
\right)^\frac12\]
Where $A:A$ is the Frobenius inner product.
If $\|A\|$ or $\|A\|_e$ will be computed for
some matrix field $A$, it may be advantageous
to store the field $A:A$.

\section{Size Field Computation}
A per-element scalar size factor is computed based on
the per-element error following this formula:
\[h^\text{new}_e = h^\text{current}_e
\|e_\epsilon\|^{-\frac{2}{2p+d}}_e
\left(
\frac
{\hat{\eta}^2\|\epsilon^*\|^2}
{\sum_{i=1}^n\|e_\epsilon\|^\frac{2d}{2p+d}_i}
\right)^\frac{1}{2p}
\]
To which the following definitions apply:

\begin{center}
\begin{tabular}{ll}
$d$ & element dimension \\
$p$ & polynomial order of $f$ \\
$\eta$ & $\frac{\|e_\epsilon\|}{\|\epsilon\|}$ \\
$\hat{\eta}$ & threshold on $\eta$ for adaptivity \\
$n$ & number of elements in the mesh \\
$h_e^\text{current}$ & the current element size (longest edge) \\
$h_e^\text{new}$ & the desired element size for adaptivity \\
\end{tabular}
\end{center}

MeshAdapt expects a linear size field, which in this
case is isotropic, so the desired element size values
must be converted into vertex size field values.
The current code simply averages the desired sizes
of elements adjacent to a vertex.

Interestingly, the desired element sizes can be seen
as a step-wise field, and vertex value computation
is then a recovery procedure.
It is worth considering whether a simple average
is a sufficient recovery procedure for this step.

\appendix
\section{Singular Value Decomposition}
\[A=UWV^T\]
\[\text{find}\ a\ \text{that minimizes}\ \chi^2=|A\cdot a-b|^2\]
\[a = \sum_{i=0}^{M-1}\left(\frac{U_{(i)}\cdot b}{w_i}\right)V_{(i)}\]
$U_{(i)}$ are columns of $U$, $w_i=W_{ii}$.

\section{SPR}

To recover the nodal value of a C-1 field, the first
step is to obtain field values at key points.
The points chosen are the gaussian quadrature integration
points of surrounding regions.

For each component of the tensor, that component is looked
at as a scalar field and a polynomial approximation is
fitted to the known point values.
Let $n$ be the number of points, $\epsilon_i^h$ be the
original component value at point $i$, $(x_i,y_i,z_i)$
are the coordinates of point $i$.
\[P(x,y,z)=
\begin{bmatrix}
1 & x & y & z
\end{bmatrix}
\]
Fitting the polynomial requires solving $Aa=b$ where $a$
is the vector of polynomial coefficients and:
\[A=\sum_{i=1}^nP^T(x_i,y_i,z_i)P(x_i,y_i,z_i)\]
\[b=\sum_{i=1}^nP^T(x_i,y_i,z_i)\epsilon_i^h(x_i,y_i,z_i)\]

In order to have enough points for the solution to be
well-defined, we need at least as many points as
polynomial coefficients.

To collect those points, we start with all the elements
adjacent to the node.
If those elements don't have enough integration points,
we expand the patch to include adjacent elements, using
the following adjacencies in order:
\begin{enumerate}
\item add elements adjacent to faces of the patch
\item add elements adjacent to edges of the patch
\item add elements adjacent to vertices of the patch
\end{enumerate}
These additions are tried in order repeatedly until
enough points are collected.
There is the possibility of insufficent elements
in the mesh part, in which case the whole program fails.

The patch is first collected and expanded, then the points
are used to fit polynomials for each component.
To get the nodal value, the polynomials are evaluated
at the nodal coordinates.

\end{document}
